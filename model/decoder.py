'''
Decoder model is used to decode the tokens generated by the Fourier coefficients model back to the encoded image encoded by the vae model.
The encoded  can be sampled to reconstruct the original image using the vae model decoder.
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
from timm.models.vision_transformer import PatchEmbed, Attention
import numpy as np
from .encoder import ViT_block, LabelEmbedder, FinalLayer, get_2d_sincos_pos_embed
from .FN_block import FourierSeries_Reconstruction
    
    
class Decoder(nn.Module):
    '''
    Decoder model is used to decode the tokens generated by the Fourier coefficients model back to the encoded image encoded by the vae model.
    Use the same architecture as the Vision Transformer model in the encoder.
    '''
    def __init__(self, output_size=32, out_channels=4, 
                 num_classes=1000, num_fourier_terms=512,
                 hidden_size=1024, depth=12, num_heads=16, 
                 mlp_ratio=4.0, patch_size=4, 
                 num_patches=64):
        super(Decoder, self).__init__()
        self.output_size = output_size
        self.out_channels = out_channels
        self.num_classes = num_classes
        self.num_fourier_terms = num_fourier_terms
        self.hidden_size = hidden_size
        self.depth = depth
        self.num_heads = num_heads
        self.mlp_ratio = mlp_ratio
        self.patch_size = patch_size
        
        # Embed 
        self.label_embedder = LabelEmbedder(num_classes, hidden_size)
        self.num_patches = num_patches if num_patches == (output_size // patch_size) ** 2 else (output_size // patch_size) ** 2
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)
        
        self.blocks = nn.ModuleList([
            ViT_block(hidden_size, num_heads, mlp_ratio) for _ in range(depth)
        ])
        self.initilize_parameters()
    
    def initilize_parameters(self):
        # Initialize transformer layers:
        def _basic_init(module):
            if isinstance(module, nn.Linear):
                torch.nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.constant_(module.bias, 0)
        self.apply(_basic_init)

        # Initialize (and freeze) pos_embed by sin-cos embedding:
        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.num_patches ** 0.5))
        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
        
        # Initialize label embedding table:
        nn.init.normal_(self.label_embedder.embedding_table.weight, std=0.02)
        
        # Zero-out adaLN modulation layers in DiT blocks:
        for block in self.blocks:
            nn.init.constant_(block.module[-1].weight, 0)
            nn.init.constant_(block.module[-1].bias, 0)
        
    def unpatchify(self, x):
        """
        x: (N, T, patch_size**2 * C)
        imgs: (N, H, W, C)
        """
        c = self.out_channels
        p = self.num_patches
        h = w = int(x.shape[1] ** 0.5)
        assert h * w == x.shape[1]

        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))
        x = torch.einsum('nhwpqc->nchpwq', x)
        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))
        return imgs

    def forward(self, x, labels):
        # x: (N, T, hidden_size) -> (N, T, patch_size**2 * out_channels) -> (N, out_channels, H, W)
        x = x + self.pos_embed
        c = self.label_embedder(labels) # (B, hidden_size)
        for block in self.blocks:
            x = block(x, c)
        x = self.unpatchify(x)
        return x


#################################################################################
#                                   ViT Configs                                  #
#################################################################################
def Dec_XL_2(**kwargs):
    return Decoder(depth=24, num_heads=16, hidden_size=1152, patch_size=2, **kwargs)

def Dec_XL_4(**kwargs):
    return Decoder(depth=24, num_heads=16, hidden_size=1152, patch_size=4, **kwargs)

def Dec_XL_8(**kwargs):
    return Decoder(depth=24, num_heads=16, hidden_size=1152, patch_size=8, **kwargs)

def Dec_L_2(**kwargs):
    return Decoder(depth=12, num_heads=8, hidden_size=1024, patch_size=2, **kwargs)

def Dec_L_4(**kwargs):
    return Decoder(depth=12, num_heads=8, hidden_size=1024, patch_size=4, **kwargs)

def Dec_L_8(**kwargs):
    return Decoder(depth=12, num_heads=8, hidden_size=1024, patch_size=8, **kwargs)

def Dec_B_2(**kwargs):
    return Decoder(depth=12, num_heads=12, hidden_size=768, patch_size=2, **kwargs)

def Dec_B_4(**kwargs):
    return Decoder(depth=12, num_heads=12, hidden_size=768, patch_size=4, **kwargs)

def Dec_B_8(**kwargs):
    return Decoder(depth=12, num_heads=12, hidden_size=768, patch_size=8, **kwargs)

Dec_models = {
    'Dec_XL_2': Dec_XL_2,
    'Dec_XL_4': Dec_XL_4,
    'Dec_XL_8': Dec_XL_8,
    'Dec_L_2': Dec_L_2,
    'Dec_L_4': Dec_L_4,
    'Dec_L_8': Dec_L_8,
    'Dec_B_2': Dec_B_2,
    'Dec_B_4': Dec_B_4,
    'Dec_B_8': Dec_B_8,
}